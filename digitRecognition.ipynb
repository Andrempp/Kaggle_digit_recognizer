{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digitRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vlrJpYIr1KM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Participation on the Digit Recognizer competition using Pytorch - accuracy of 0.98271\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Notebook for my first participation on a Kaggle competition.\n",
        "Used the kernels [Introduction to CNN Keras - Acc 0.997 (top 8%)](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6) and [PyTorch Dataset and DataLoader](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader) as inspiration."
      ]
    },
    {
      "metadata": {
        "id": "8ZklJBi-2jbz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import and installation of the necessary resources to download the data and save trained models on Google Drive."
      ]
    },
    {
      "metadata": {
        "id": "8cZyB8iIcwcL",
        "colab_type": "code",
        "outputId": "c26ae9cf-0602-48b4-fa09-c1328b54efff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = '/content/drive/My Drive/Colab Notebooks'\n",
        "drive_full_path = drive_path + '/K_Digit_Recognizer'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IqY3b4naGWRQ",
        "colab_type": "code",
        "outputId": "5fd34092-c843-461c-a545-776606ad675a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "drive_full_path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/K_Digit_Recognizer'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "8Cd370gE-liV",
        "colab_type": "code",
        "outputId": "083b7f68-fd1a-43e5-abe4-90333f693ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GKJ7foSS_FUU",
        "colab_type": "code",
        "outputId": "db581bbf-05b4-4942-f427-c1d8e319eeb5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-678adcae-dec7-437e-ba91-dd15acdff891\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-678adcae-dec7-437e-ba91-dd15acdff891\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "82agnNsE3AQ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import the authentication file from Kaggle."
      ]
    },
    {
      "metadata": {
        "id": "r6Ic8kH9BXip",
        "colab_type": "code",
        "outputId": "ba8cc709-2907-489a-a4c8-3e7a620b31b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ref                                                deadline             category            reward  teamCount  userHasEntered  \n",
            "-------------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "digit-recognizer                                   2030-01-01 00:00:00  Getting Started  Knowledge       2627            True  \n",
            "titanic                                            2030-01-01 00:00:00  Getting Started  Knowledge      10762            True  \n",
            "house-prices-advanced-regression-techniques        2030-01-01 00:00:00  Getting Started  Knowledge       4442            True  \n",
            "imagenet-object-localization-challenge             2029-12-31 07:00:00  Research         Knowledge         34           False  \n",
            "competitive-data-science-predict-future-sales      2019-12-31 23:59:00  Playground           Kudos       2649           False  \n",
            "two-sigma-financial-news                           2019-07-15 23:59:00  Featured          $100,000       2927           False  \n",
            "aerial-cactus-identification                       2019-07-08 23:59:00  Playground       Knowledge        237           False  \n",
            "jigsaw-unintended-bias-in-toxicity-classification  2019-06-26 23:59:00  Featured           $65,000        247           False  \n",
            "inaturalist-2019-fgvc6                             2019-06-10 23:59:00  Research             Kudos         14           False  \n",
            "iwildcam-2019-fgvc6                                2019-06-07 23:59:00  Playground           Kudos         42           False  \n",
            "imet-2019-fgvc6                                    2019-06-04 23:59:00  Research             Kudos        115           False  \n",
            "LANL-Earthquake-Prediction                         2019-06-03 23:59:00  Research           $50,000       2243           False  \n",
            "tmdb-box-office-prediction                         2019-05-30 23:59:00  Playground       Knowledge        577           False  \n",
            "dont-overfit-ii                                    2019-05-07 23:59:00  Playground            Swag       1420           False  \n",
            "ciphertext-challenge-ii                            2019-04-25 23:59:00  Playground            Swag         24           False  \n",
            "data-science-for-good-careervillage                2019-04-23 23:59:00  Analytics          $15,000          0           False  \n",
            "gendered-pronoun-resolution                        2019-04-22 23:59:00  Research           $25,000        603           False  \n",
            "petfinder-adoption-prediction                      2019-04-18 23:59:00  Featured           $25,000       2010           False  \n",
            "career-con-2019                                    2019-04-11 23:59:00  Recruitment           Swag       1204           False  \n",
            "santander-customer-transaction-prediction          2019-04-10 23:59:00  Featured           $65,000       8482           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "foEwJxK_BgMp",
        "colab_type": "code",
        "outputId": "bcba5ba9-f24a-4651-99b9-bc2a223aeb4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c digit-recognizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jLeQXcST3NIJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Access all the currently active competitions and download the data from the Digit Recognizer one."
      ]
    },
    {
      "metadata": {
        "id": "6Kk9NmyvBwNP",
        "colab_type": "code",
        "outputId": "e3d1d024-ab88-4f0f-a502-ebe1f0d907d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data  sample_submission.csv  test.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oDAXv6DCCmQb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Imports for constructing the Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "#Imports for handling data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "\n",
        "#Imports for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "#import torchvision.models as models\n",
        "#from torch.utils.data.sampler import SubsetRandomSampler\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvgJiZURCnjM",
        "colab_type": "code",
        "outputId": "ba6edb7e-ab1b-42c4-a2fa-89981f519763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "train_on_gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "iGEey7wK5Q4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check if we're running on a GPU"
      ]
    },
    {
      "metadata": {
        "id": "w3n3oRkXOsHU",
        "colab_type": "code",
        "outputId": "1db42f9b-ed3a-4638-d33d-55d2a1ed5c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, height, width, channels, transform=None, test=False):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.test = test\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # load image as ndarray type (Height * Width * Channels)\n",
        "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
        "        # in this example, we use ToTensor(), so we define the numpy array like (H, W, C)\n",
        "        \n",
        "        if self.test:\n",
        "          image_numpy = self.data.iloc[index].values.astype(np.uint8).reshape(\n",
        "              (self.height, self.width))\n",
        "          transform = transforms.ToTensor()\n",
        "          image =transform(image_numpy)   #transform.toTensor already /255\n",
        "          return image\n",
        "        \n",
        "        else:\n",
        "          image_numpy = self.data.iloc[index, 1:].values.astype(np.uint8).reshape(\n",
        "              (self.height, self.width))\n",
        "          image = Image.fromarray(image_numpy.astype('uint8'))\n",
        "\n",
        "          label = int(self.data.iloc[index, 0])\n",
        "\n",
        "          if self.transform is not None:\n",
        "              image = self.transform(image)   #transform.toTensor already /255\n",
        "\n",
        "          return image, label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"class CSVDataset(Dataset):\\n    \\n    def __init__(self, data, height, width, channels,transform=None):\\n        self.data = data\\n        self.transform = transform\\n        self.height = height\\n        self.width = width\\n        self.channels = channels\\n        \\n    def __len__(self):\\n        return len(self.data)\\n    \\n    def __getitem__(self, index):\\n        # load image as ndarray type (Height * Width * Channels)\\n        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\\n        # in this example, we use ToTensor(), so we define the numpy array like (H, W, C)\\n        image_numpy = self.data.iloc[index, 1:].values.astype(np.uint8).reshape(\\n            (self.height, self.width))\\n        image = Image.fromarray(image_numpy.astype('uint8'))\\n\\n        label = int(self.data.iloc[index, 0])\\n        \\n        if self.transform is not None:\\n            image = self.transform(image)   #transform.toTensor already /255\\n            \\n        return image, label\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "G-1Rr1ny5YxR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Custom Dataset to handle images from a CSV file."
      ]
    },
    {
      "metadata": {
        "id": "cvefKWG_sj_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train.csv',dtype = np.float32)\n",
        "test_data = pd.read_csv('test.csv',dtype = np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tL4H_Kvq6COf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download the train and test data into a pandas dataframe."
      ]
    },
    {
      "metadata": {
        "id": "cngyPntR351Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ratio of data used for trainning, the rest will be used for validation\n",
        "train_ratio=0.85\n",
        "\n",
        "div = int(train_data.shape[0] * train_ratio)\n",
        "\n",
        "#trainning data\n",
        "train = train_data.iloc[:div]\n",
        "#validation data\n",
        "val = train_data.iloc[div:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z9xJ6PegiAXH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Divide the data for trainning and validation."
      ]
    },
    {
      "metadata": {
        "id": "QprqK4PfPaIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_transforms = {'train':transforms.Compose([\n",
        "                      transforms.RandomRotation(10),  #rotate the image by a random value between -10 and 10\n",
        "                      transforms.ToTensor()]) ,       #transforms the data in a torch.FloatTensor with values between 0.0 and 1.0\n",
        "                   'val':transforms.Compose([\n",
        "                          transforms.ToTensor()])\n",
        "                  }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sqs6WCGr5jAS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the transforms to apply to the trainning and validation sets."
      ]
    },
    {
      "metadata": {
        "id": "UBq6J4PfC5ry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Convert the data to datasets of 28x28x1 tensors with the corresponding transforms applied\n",
        "train_dataset = CSVDataset(train, 28, 28, 1, transform=data_transforms['train'])\n",
        "val_dataset = CSVDataset(val, 28, 28, 1, transform=data_transforms['val'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2t0V73gWjOuV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Put the data in a dataset."
      ]
    },
    {
      "metadata": {
        "id": "vdD1MKc_QkMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sXCcTucijTQO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the batch size and create data loaders used to iterate the datasets."
      ]
    },
    {
      "metadata": {
        "id": "AhpXwhH9t0y3",
        "colab_type": "code",
        "outputId": "845f379a-92ab-4a58-cd68-7c8ca83e6e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "cell_type": "code",
      "source": [
        "g = sns.countplot(train_data.label.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1428: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
            "  stat_data = remove_na(group_data)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFKCAYAAADxKk0BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Un3UFWJukIUy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Overview of the total number of images in each class."
      ]
    },
    {
      "metadata": {
        "id": "kzjCwMd8uux5",
        "colab_type": "code",
        "outputId": "268b444d-a6a9-4e65-b1ec-e94351409ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(train_dataset[25][0].view(28,28))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efcdaaee278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zFE_gHlDkbIb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example of one of the images used as input."
      ]
    },
    {
      "metadata": {
        "id": "218bPO-7IjlO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNISTNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "      # 1-> number of channels in the input (1 because it's a grayscale image)\n",
        "      # 32 -> number of kernels used, and the number of channels in the input\n",
        "      # 5 -> heigth and width of the kernels\n",
        "      # padding -> adds a padding of 2 in order to not reduce the size of the image\n",
        "      torch.nn.Conv2d(1, 32, 5, padding=2),    #convolutional layer\n",
        "      nn.ReLU(),                               #activation layer, uses Rectified Linear Unit\n",
        "      torch.nn.Conv2d(32, 32, 5, padding=2),\n",
        "      nn.ReLU(),\n",
        "      torch.nn.MaxPool2d(kernel_size=2)        #Pooling layer, chooses max value of a 2x2 square\n",
        "    ) \n",
        "    #data comes out of this layer in the shape 32x14x14\n",
        "    self.layer2 = nn.Sequential(\n",
        "      torch.nn.Conv2d(32, 64, 3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      torch.nn.Conv2d(64, 64, 3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    ) #64, 7, 7\n",
        "    self.fc1 = torch.nn.Linear(7*7*64, 256)\n",
        "    self.fc2 = torch.nn.Linear(256, 10)\n",
        "   \n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = F.dropout(x, p=0.25)                  #applies a dropout layer, with 0.25 probability of ignoring each element of the input tensor.\n",
        "    x = self.layer2(x)\n",
        "    x = F.dropout(x, p=0.25)\n",
        "    x = x.reshape(x.size(0), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, p=0.5)\n",
        "    out = F.softmax(self.fc2(x))\n",
        "    return out\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HYhryZC3k7RD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_model(model, epochs=0, val_loss=-1):\n",
        "  '''function to save the state of the model, the state of the optimizer, \n",
        "  number of current epochs and validation loss'''\n",
        "  \n",
        "  checkpoint = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), \n",
        "              'epochs': epochs, 'val_loss': val_loss}\n",
        "  torch.save(checkpoint, drive_full_path + '/checkpoint.pt')\n",
        "  print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RF8vYx-ourM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_model(checkpoint):\n",
        "    '''function to load a model and a optimizer with the respective state dicts,\n",
        "    returning them and the lowest validation loss associated with them'''\n",
        "    model = MNISTNetwork()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    val_loss = checkpoint['val_loss']\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    \n",
        "    return model, optimizer, val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ey9ML1hR3Hzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "try:                   #if there is a file with a checkpoint of trainning load it and use its data\n",
        "  \n",
        "    #load te checkpoint from the file located in Google Drive\n",
        "    checkpoint = torch.load(drive_full_path + '/checkpoint.pt')\n",
        "    \n",
        "    model, optimizer, val_loss = load_model(checkpoint)\n",
        "    \n",
        "    # track change in validation loss\n",
        "    valid_loss_min = val_loss\n",
        "    \n",
        "    \n",
        "except FileNotFoundError:   #if there is no file with a checkpoint of trainning, start fresh\n",
        "\n",
        "    #instatiate the model\n",
        "    model = MNISTNetwork()\n",
        "\n",
        "    #define the optimizer as Adam, give him the model's parameters and define the learning rate as 0.001\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # track change in validation loss\n",
        "    valid_loss_min = np.Inf\n",
        "    \n",
        "\n",
        "#define the loss as Cross Entropy Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#schedule the lr to decrease by half after every 3 epochs without improvement\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3, verbose=True, min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fa_u8aalv6UT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Instiatiate the model and create the necessary resources necessary to train it."
      ]
    },
    {
      "metadata": {
        "id": "JBW0_JFTbcIR",
        "colab_type": "code",
        "outputId": "7e776b94-a83f-430a-8228-0a95be468e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3264
        }
      },
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 25\n",
        "\n",
        "# number of epochs without improvement that triggers an early stop in trainning\n",
        "early_stop = 7\n",
        "es_counter = 0\n",
        "\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    accuracy=0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for batch_number, (data, target) in enumerate(train_loader):\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        #give feedback on current batch of the epoch\n",
        "        if batch_number%50 == 0:\n",
        "            print(\"batch number: {}\".format(batch_number))\n",
        "            \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for data, target in val_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        _,pred=torch.max(output,1)\n",
        "        accuracy += torch.sum(pred==target.data)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    valid_loss = valid_loss/len(val_loader.dataset)\n",
        "    accuracy = accuracy.double()/len(val_loader.dataset)\n",
        "    \n",
        "    # confirm if the validation loss is decreasing in order to reduce lr\n",
        "    scheduler.step(valid_loss)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\\tAccuracy: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss,accuracy))\n",
        "    \n",
        "    # save model if validation loss has decreased, if not increase early stop counter \n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min, valid_loss))\n",
        "        save_model(model, epoch, valid_loss)\n",
        "        valid_loss_min = valid_loss\n",
        "        es_counter = 0\n",
        "    else:\n",
        "        es_counter+=1\n",
        "    \n",
        "    #if certain number of epochs have passed without improvement stop the trainning\n",
        "    if es_counter >= early_stop:\n",
        "        print(\"\\n\\nEarly stop, no improvements in {} epochs\".format(early_stop))\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 1 \tTraining Loss: 1.498680 \tValidation Loss: 1.489930\tAccuracy: 0.971270\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 2 \tTraining Loss: 1.494231 \tValidation Loss: 1.488496\tAccuracy: 0.972222\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 3 \tTraining Loss: 1.493846 \tValidation Loss: 1.491277\tAccuracy: 0.969841\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 4 \tTraining Loss: 1.494865 \tValidation Loss: 1.486612\tAccuracy: 0.974762\n",
            "Validation loss decreased (1.488192 --> 1.486612).  Saving model ...\n",
            "Model Saved\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 5 \tTraining Loss: 1.491709 \tValidation Loss: 1.489294\tAccuracy: 0.971587\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 6 \tTraining Loss: 1.491965 \tValidation Loss: 1.491081\tAccuracy: 0.970000\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 7 \tTraining Loss: 1.491969 \tValidation Loss: 1.486210\tAccuracy: 0.974762\n",
            "Validation loss decreased (1.486612 --> 1.486210).  Saving model ...\n",
            "Model Saved\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 8 \tTraining Loss: 1.493679 \tValidation Loss: 1.485051\tAccuracy: 0.976349\n",
            "Validation loss decreased (1.486210 --> 1.485051).  Saving model ...\n",
            "Model Saved\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 9 \tTraining Loss: 1.492405 \tValidation Loss: 1.486664\tAccuracy: 0.974762\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 10 \tTraining Loss: 1.493323 \tValidation Loss: 1.486194\tAccuracy: 0.974921\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 11 \tTraining Loss: 1.492160 \tValidation Loss: 1.488719\tAccuracy: 0.972063\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch    11: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch: 12 \tTraining Loss: 1.493111 \tValidation Loss: 1.489925\tAccuracy: 0.970952\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 13 \tTraining Loss: 1.486506 \tValidation Loss: 1.485410\tAccuracy: 0.975873\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 14 \tTraining Loss: 1.484583 \tValidation Loss: 1.483178\tAccuracy: 0.978095\n",
            "Validation loss decreased (1.485051 --> 1.483178).  Saving model ...\n",
            "Model Saved\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 15 \tTraining Loss: 1.485449 \tValidation Loss: 1.484837\tAccuracy: 0.976032\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 16 \tTraining Loss: 1.484370 \tValidation Loss: 1.481234\tAccuracy: 0.980000\n",
            "Validation loss decreased (1.483178 --> 1.481234).  Saving model ...\n",
            "Model Saved\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 17 \tTraining Loss: 1.484115 \tValidation Loss: 1.485395\tAccuracy: 0.975714\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 18 \tTraining Loss: 1.483609 \tValidation Loss: 1.484998\tAccuracy: 0.975714\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 19 \tTraining Loss: 1.483822 \tValidation Loss: 1.480628\tAccuracy: 0.980635\n",
            "Validation loss decreased (1.481234 --> 1.480628).  Saving model ...\n",
            "Model Saved\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 20 \tTraining Loss: 1.481112 \tValidation Loss: 1.480914\tAccuracy: 0.980317\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 21 \tTraining Loss: 1.484126 \tValidation Loss: 1.483787\tAccuracy: 0.977302\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 22 \tTraining Loss: 1.481130 \tValidation Loss: 1.480446\tAccuracy: 0.980476\n",
            "Validation loss decreased (1.480628 --> 1.480446).  Saving model ...\n",
            "Model Saved\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 23 \tTraining Loss: 1.481238 \tValidation Loss: 1.480970\tAccuracy: 0.980159\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 24 \tTraining Loss: 1.482320 \tValidation Loss: 1.480653\tAccuracy: 0.980476\n",
            "batch number: 0\n",
            "batch number: 50\n",
            "batch number: 100\n",
            "batch number: 150\n",
            "batch number: 200\n",
            "batch number: 250\n",
            "Epoch: 25 \tTraining Loss: 1.481905 \tValidation Loss: 1.480539\tAccuracy: 0.980635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lwsRZJP5U0KF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dataset = CSVDataset(test_data, 28, 28, 1, test=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPh132C5ENUq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = pd.DataFrame(columns = ['ImageId', 'Label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrnC3D-5Vxs2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(drive_full_path + '/checkpoint.pt')\n",
        "\n",
        "model, _, _ = load_model(checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_tlJlWpA9g4",
        "colab_type": "code",
        "outputId": "79991607-fd82-486c-bf4f-2c8abe7dfaf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "image_id = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    image_id+=1\n",
        "    output = model(data)\n",
        "    _,pred=torch.max(output,1)\n",
        "    \n",
        "    #row = {'ImageId' : image_id, 'Label' : pred}\n",
        "    serie = pd.Series([image_id, int(pred)],index=predictions.columns)\n",
        "    predictions = predictions.append(serie, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XGkojRE0NBuk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions.to_csv('submission.csv', index=False)\n",
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}